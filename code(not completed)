import os
import cv2
import random
import uuid
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf

# Import TensorFlow dependencies
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten

## 1.3 Set GPU Growth
# Set memory growth to avoid out-of-memory errors
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

## 1.4 Create Folder Structures
# Setup paths
POS_PATH = os.path.join('data', 'positive')
NEG_PATH = os.path.join('data', 'negative')
ANC_PATH = os.path.join('data', 'anchor')

# Create directories if they donï¿½t exist
os.makedirs(POS_PATH, exist_ok=True)
os.makedirs(NEG_PATH, exist_ok=True)
os.makedirs(ANC_PATH, exist_ok=True)

# 2. Collect Positives and Anchors

## 2.1 Untar Labelled Faces in the Wild Dataset
# Ensure dataset is downloaded from http://vis-www.cs.umass.edu/lfw/


# Move LFW Images to the negative folder
for directory in os.listdir('lfw'):
    for file in os.listdir(os.path.join('lfw', directory)):
        EX_PATH = os.path.join('lfw', directory, file)
        NEW_PATH = os.path.join(NEG_PATH, file)
        os.replace(EX_PATH, NEW_PATH)

## 2.2 Collect Positive and Anchor Classes
# Function to capture images from the webcam
cap = cv2.VideoCapture(0)  # Use correct webcam index
while cap.isOpened(): 
    ret, frame = cap.read()
    if not ret:
        break

    # Ensure frame has enough resolution before cropping
    if frame.shape[0] > 370 and frame.shape[1] > 450:
        frame = frame[120:120+250, 200:200+250, :]

    # Show image back to screen
    cv2.imshow('Image Collection', frame)

    # Collect anchors (press 'a')
    if cv2.waitKey(1) & 0XFF == ord('a'):
        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))
        cv2.imwrite(imgname, frame)

    # Collect positives (press 'p')
    if cv2.waitKey(1) & 0XFF == ord('p'):
        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))
        cv2.imwrite(imgname, frame)

    # Break gracefully (press 'q')
    if cv2.waitKey(1) & 0XFF == ord('q'):
        break

# Release the webcam and close the image display
cap.release()
cv2.destroyAllWindows()

# 2.x NEW - Data Augmentation

def data_aug(img):
    data = []
    for i in range(9):
        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1, 2))
        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1, 3))
        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100), np.random.randint(100)))
        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100), np.random.randint(100)))
        img = tf.image.stateless_random_saturation(img, lower=0.9, upper=1, seed=(np.random.randint(100), np.random.randint(100)))
        data.append(img)
    return data

# Apply augmentation to images
for folder_path in [ANC_PATH, POS_PATH]:
    for file_name in os.listdir(folder_path):
        img_path = os.path.join(folder_path, file_name)
        img = cv2.imread(img_path)
        if img is not None:
            img = tf.convert_to_tensor(img)
            augmented_images = data_aug(img)
            for image in augmented_images:
                cv2.imwrite(os.path.join(folder_path, '{}.jpg'.format(uuid.uuid1())), image.numpy())

# 3. Load and Preprocess Images

## 3.1 Get Image Directories
anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, '*.jpg')).take(3000)
positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, '*.jpg')).take(3000)
negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, '*.jpg')).take(3000)

## 3.2 Preprocessing - Scale and Resize
def preprocess(file_path):
    # Read in image from file path
    byte_img = tf.io.read_file(file_path)
    img = tf.io.decode_jpeg(byte_img)
    # Preprocessing steps - resizing the image to be 100x100x3
    img = tf.image.resize(img, (100, 100))
    # Scale image to be between 0 and 1
    img = img / 255.0
    return img

# Test preprocessing
sample_img = preprocess(next(anchor.as_numpy_iterator()))

## 3.3 Create Labelled Dataset
positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))
negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))
data = positives.concatenate(negatives)

# 3.4 Build Train and Test Partition
def preprocess_twin(input_img, validation_img, label):
    return (preprocess(input_img), preprocess(validation_img), label)

# Apply the preprocess_twin function to the dataset
data = data.map(preprocess_twin)

# Split into train and test sets
data = data.shuffle(buffer_size=1024)
train = data.take(round(len(data) * 0.7))
test = data.skip(round(len(data) * 0.7))
